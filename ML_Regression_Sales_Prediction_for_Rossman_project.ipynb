{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/humayel1/Sales-Prediction-for-major-chain-store-Rossman/blob/main/ML_Regression_Sales_Prediction_for_Rossman_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Regression - Retail Sales Prediction: Predicting sales of a major store chain Rossmann\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member1 - Humayel**\n",
        "##### **Team Member2 - Divyanshu Chauhan**\n",
        "##### **Team Member3 - Shubham**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In seven European nations, Rossmann runs more than 3,000 pharmacies. Store managers at Rossmann are currently required to forecast their daily sales up to six weeks ahead of time. Numerous factors, such as competition, school and state holidays, seasonality, locality, and promotions, affect store sales. The accuracy of the statistics can vary greatly since thousands of managers forecast sales based on their own set of conditions.\n",
        "Historical sales information for 1,115 Rossmann establishments is given to you. Predicting the \"Sales\" column for the test set is the task. It should be noted that a few of the dataset's stores were briefly closed for renovations."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description\n",
        "\n",
        "#### Rossmann Stores Data.csv - historical data including Sales\n",
        "#### store.csv  - supplemental information about the stores\n",
        "\n",
        "\n",
        "#### <u>Data fields</u>\n",
        "#### Most of the fields are self-explanatory.\n",
        "\n",
        "* **Id** - an Id that represents a (Store, Date) duple within the set\n",
        "*  **Store** - a unique Id for each store\n",
        "*  **Sales** - the turnover for any given day (Dependent Variable)\n",
        "* **Customers** - the number of customers on a given day\n",
        "* **Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "* **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
        "* **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "* **StoreType** - differentiates between 4 different store models: a, b, c, d\n",
        "* **Assortment** - describes an assortment level: a = basic, b = extra, c = extended. An assortment strategy in retailing involves the number and type of products that stores display for purchase by consumers.\n",
        "* **CompetitionDistance** - distance in meters to the nearest competitor store\n",
        "* **CompetitionOpenSince**[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "* **Promo** - indicates whether a store is running a promo on that day\n",
        "* **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "* **Promo2Since**[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "* **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
      ],
      "metadata": {
        "id": "jFeEn7y2DGcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/humayel1/Sales-Prediction-for-magor-chain-store-Rossman"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product interest continues to fluctuate from time to time. No company can focus on financial expansion without accurately determining the interest of its customers and the future demand for its products. The process of projecting demand or sales of a specific product over a given time period is known as sales forecasting. In this project, a machine learning model for sales forecasting will be developed and a real-world business problem will be solved.\n",
        "\n",
        "Here, our objective is to project each store's sales for the next six weeks, identify the variables that affect it, and offer suggestions for raising the figures.\n",
        "\n",
        "####Approach\n",
        "* Business Problem\n",
        "* Data Collection and Preprocessing\n",
        "      - Importing important libraries and modules\n",
        "      - Data Cleaning\n",
        "      - Missing Data Handling\n",
        "      - Merging the Datasets\n",
        "* Exploratory Data Analysis\n",
        "      - Hypotheses\n",
        "      - Categorical Features\n",
        "      - Continuous Features\n",
        "      - EDA Conclusion and Validating Hypotheses\n",
        "* Feature Selection and Outlier Detection\n",
        "      - Feature Engineering\n",
        "      - Outlier Detection and Treatment\n",
        "* Modeling\n",
        "      - Train Test Split\n",
        "      - Baseline Model - Decision Tree\n",
        "      - Random Forest Model\n",
        "      - Random Forest Hyperparameter Tuning\n",
        "      - Random Forest Feature Importance\n",
        "* Model Performance and Evaluation\n",
        "        - Visualizing Model Performances\n",
        "        - Random Forest vs Baseline Model\n",
        "        - Random Forest Tuned vs Baseline and Random Forest Models\n",
        "* Store wise Sales Predictions\n",
        "* Conclusion and Recommendations"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing important libraries and modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.rcParams.update({'figure.figsize':(8,5),'figure.dpi':100})\n",
        "from datetime import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_DzLwAkxHlbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "#reading the csv file and converting it to pandas dataframes\n",
        "sales_df = pd.read_csv(\"/content/drive/MyDrive/Rossman ML Training, Test dataset/Rossmann Stores Data.csv\",parse_dates=['Date'])\n",
        "stores_df = pd.read_csv(\"/content/drive/MyDrive/store.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we have historical sales data and stores datasets\n",
        "#first look of the sales data\n",
        "sales_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNlne9wOnTO1"
      },
      "outputs": [],
      "source": [
        "#first look of stores dataframe\n",
        "stores_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYyWlBRGnadD"
      },
      "outputs": [],
      "source": [
        "#info about the sales dataset\n",
        "sales_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are  1017209 rows or observations and 9 columns in this dataset. There seems to be no null values in it. It has integer, datetime and object as data types."
      ],
      "metadata": {
        "id": "Zmlbpc13KH-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOoMMk8ypl1A"
      },
      "outputs": [],
      "source": [
        "#info about the stores\n",
        "stores_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 1115 rows and 10 columns. There are missing values in it and it is important to impute them with appropriate values in order to get good results later on."
      ],
      "metadata": {
        "id": "ik1RTgFfKPcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicates1 = sales_df[sales_df.duplicated()]\n",
        "print(duplicates1.sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates2 = stores_df[stores_df.duplicated()]\n",
        "print(duplicates2.sum())"
      ],
      "metadata": {
        "id": "6UVntpxwLgb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#null values in stores df\n",
        "stores_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of 1115 entries there are missing values for the columns:\n",
        "* CompetitionDistance- distance in meters to the nearest competitor store, the distribution plot would give us an idea about the distances at which generally the stores are opened and we would impute the values accordingly.\n",
        "\n",
        "* CompetitionOpenSinceMonth- gives the approximate month of the time the nearest competitor was opened, mode of the column would tell us the most occuring month    \n",
        "* CompetitionOpenSinceYear-  gives the approximate year of the time the nearest competitor was opened, mode of the column would tell us the most occuring month    \n",
        "* Promo2SinceWeek, Promo2SinceYear and PromoInterval are NaN wherever Promo2 is 0 or False as can be seen in the first look of the dataset. They can be replaced with 0.      \n"
      ],
      "metadata": {
        "id": "-GDGzPewL9kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "#distribution plot of competition distance\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.distplot(x=stores_df['CompetitionDistance'], hist=True, color='red')\n",
        "plt.xlabel('Competition Distance Distribution Plot')"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like most of the values of the CompetitionDistance are towards the left and the distribution is skewed on the right. Median is more robust to outlier effect."
      ],
      "metadata": {
        "id": "cEs-mlVgMiwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition distance with the median value\n",
        "stores_df['CompetitionDistance'].fillna(stores_df['CompetitionDistance'].median(), inplace = True)"
      ],
      "metadata": {
        "id": "sYIgdQ4cMn-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition open since month and year with the most occuring values of the columns i.e modes of those columns\n",
        "stores_df['CompetitionOpenSinceMonth'].fillna(stores_df['CompetitionOpenSinceMonth'].mode()[0], inplace = True)\n",
        "stores_df['CompetitionOpenSinceYear'].fillna(stores_df['CompetitionOpenSinceYear'].mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "Rp9MqcekMuJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing the nan values of promo2 related columns with 0\n",
        "stores_df['Promo2SinceWeek'].fillna(value=0,inplace=True)\n",
        "stores_df['Promo2SinceYear'].fillna(value=0,inplace=True)\n",
        "stores_df['PromoInterval'].fillna(value=0,inplace=True)"
      ],
      "metadata": {
        "id": "3H9sVR-EMxPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "stores_df.isnull().sum()"
      ],
      "metadata": {
        "id": "EUMDv9teM0pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Merging the two Datasets"
      ],
      "metadata": {
        "id": "H7z4pz9YNU10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#merge the datasets on stores data\n",
        "df = sales_df.merge(right=stores_df, on=\"Store\", how=\"left\")"
      ],
      "metadata": {
        "id": "Ro5iqRRPNTGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first five rows of the merged dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cksqL074Na9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of the dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "sMK37wuINjYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 1017209 rows or observations and 9 columns in this dataset Sales_df. There seems to be no null values in it. It has integer, datetime and object as data types.\n",
        "\n",
        "**Size of the Dataset stores_df**: The dataset contains 1115 entries (rows) and 10 columns.\n",
        "\n",
        "**Data Types**: The dataset includes columns with integer, datetime, and object data types.\n",
        "\n",
        "**Missing Values**: There are missing values in several columns:\n",
        "\n",
        "* **CompetitionDistance**: Missing values need to be imputed based on the distribution of distances. The distribution is skewed to the right, with most values concentrated towards the left, so using the median for imputation is suggested.\n",
        "* **CompetitionOpenSinceMonth and CompetitionOpenSinceYear**: Missing values can be imputed with the mode (most frequent) values of each respective column.\n",
        "* **Promo2SinceWeek, Promo2SinceYear, and PromoInterval: These columns have NaN values wherever Promo2 is 0 or False. They can be replaced with 0.**\n",
        "\n",
        "**Imputation Strategy:**\n",
        "\n",
        "* For **CompetitionDistance**, using the median is preferred due to the skewness of the distribution.\n",
        "* For **CompetitionOpenSinceMonth and CompetitionOpenSinceYear**, imputing with the mode is suggested.\n",
        "* For **Promo2SinceWeek, Promo2SinceYear, and PromoInterval, NaN values can be replaced with 0 where Promo2** is 0 or False.\n",
        "\n",
        "**Data Exploration Opportunity**: Exploring the distribution of CompetitionDistance can provide insights into the distance at which stores are typically opened, helping to inform the imputation strategy for missing values.\n",
        "\n",
        "Overall, these details give us a good understanding of the dataset and the necessary steps to handle missing values effectively before proceeding with further analysis or modeling."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#datatypes\n",
        "df.info()"
      ],
      "metadata": {
        "id": "PR1K3mD4N4RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a categorical column list\n",
        "categorical_variables = ['DayOfWeek','Open','Promo','StateHoliday','SchoolHoliday','StoreType','Assortment','CompetitionOpenSinceMonth',\n",
        "                         'CompetitionOpenSinceYear','Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval']"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the unique values involved and if all the values are in appropriate datatypes\n",
        "for col in categorical_variables:\n",
        "  print(f'Unique values for {col}: {df[col].unique()}')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change into int type\n",
        "df['StateHoliday'].replace({'0':0}, inplace=True)"
      ],
      "metadata": {
        "id": "QFjiv4q_OgK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Extracting from the Date\n"
      ],
      "metadata": {
        "id": "NuTl39YHCUrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify columns with non-numeric values\n",
        "non_numeric_cols = sales_df.select_dtypes(exclude=['number']).columns\n",
        "print(\"Columns with non-numeric values:\", non_numeric_cols)\n",
        "\n",
        "# Check for unique values in non-numeric columns\n",
        "for col in non_numeric_cols:\n",
        "    unique_values = sales_df[col].unique()\n",
        "    print(f\"Unique values in column '{col}':\", unique_values)\n",
        "\n",
        "# For example, if the non-numeric values represent missing values, you can replace them with NaN:\n",
        "sales_df[non_numeric_cols] = sales_df[non_numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Then, you can drop rows with NaN values or impute missing values as needed.\n",
        "\n",
        "# After handling non-numeric values, proceed with data preprocessing, model training, and evaluation as usual."
      ],
      "metadata": {
        "id": "hZi6Utqnz0kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert non-numeric values to numeric\n",
        "holiday_mapping = {'0': 0, 'a': 1, 'b': 2, 'c': 3}\n",
        "sales_df['StateHoliday'] = sales_df['StateHoliday'].map(holiday_mapping).fillna(0)\n",
        "\n",
        "# Check if conversion is successful\n",
        "print(\"Unique values in 'StateHoliday' after conversion:\", sales_df['StateHoliday'].unique())\n"
      ],
      "metadata": {
        "id": "L0b_2iEaz0WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.to_csv('/content/drive/MyDrive/Rossman ML Training, Test dataset/Rossmann Stores Data.csv', index=False)"
      ],
      "metadata": {
        "id": "8bDM-ryl0J--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating features from the date\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['WeekOfYear'] = df['Date'].dt.weekofyear\n",
        "df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "years = df['Year'].unique()"
      ],
      "metadata": {
        "id": "Zi6UINkyCWRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save DataFrame to cleaned.csv\n",
        "df.to_csv(\"/content/drive/MyDrive/Rossman ML Training, Test dataset/cleaned.csv\", index=False)"
      ],
      "metadata": {
        "id": "MUQa1qT1nHUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# filling competition distance with the median value\n",
        "stores_df['CompetitionDistance'].fillna(stores_df['CompetitionDistance'].median(), inplace = True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition open since month and year with the most occuring values of the columns i.e modes of those columns\n",
        "stores_df['CompetitionOpenSinceMonth'].fillna(stores_df['CompetitionOpenSinceMonth'].mode()[0], inplace = True)\n",
        "stores_df['CompetitionOpenSinceYear'].fillna(stores_df['CompetitionOpenSinceYear'].mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "vClORTLyavQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing the nan values of promo2 related columns with 0\n",
        "stores_df['Promo2SinceWeek'].fillna(value=0,inplace=True)\n",
        "stores_df['Promo2SinceYear'].fillna(value=0,inplace=True)\n",
        "stores_df['PromoInterval'].fillna(value=0,inplace=True)"
      ],
      "metadata": {
        "id": "DZ2nelu2azp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge the datasets on stores data\n",
        "df = sales_df.merge(right=stores_df, on=\"Store\", how=\"left\")"
      ],
      "metadata": {
        "id": "QnisLQfEa3q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a categorical column list\n",
        "categorical_variables = ['DayOfWeek','Open','Promo','StateHoliday','SchoolHoliday','StoreType','Assortment','CompetitionOpenSinceMonth',\n",
        "                         'CompetitionOpenSinceYear','Promo2','Promo2SinceWeek','Promo2SinceYear','PromoInterval']"
      ],
      "metadata": {
        "id": "805Dplw5a7lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the unique values involved and if all the values are in appropriate datatypes\n",
        "for col in categorical_variables:\n",
        "  print(f'Unique values for {col}: {df[col].unique()}')"
      ],
      "metadata": {
        "id": "FZbpyTwxa-S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change into int type\n",
        "df['StateHoliday'].replace({'0':0}, inplace=True)"
      ],
      "metadata": {
        "id": "441iKSMHbBhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating features from the date\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['WeekOfYear'] = df['Date'].dt.weekofyear\n",
        "df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "years = df['Year'].unique()"
      ],
      "metadata": {
        "id": "IsMzJeOBbEIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Categorical Features:"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the categorical variables\n",
        "categorical_variables = ['DayOfWeek', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
        "                         'CompetitionOpenSinceMonth', 'Promo2', 'Promo2SinceYear', 'PromoInterval']\n",
        "\n",
        "# Set a color palette\n",
        "palette = 'hsv'  # You can choose any palette you prefer\n",
        "\n",
        "# Iterate over the categorical variables and plot each of them\n",
        "for value in categorical_variables:\n",
        "    plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "    ax = sns.barplot(x=value, y='Sales', data=df, palette=palette)  # Use specified palette\n",
        "    totals = []\n",
        "    for i in ax.patches:\n",
        "        totals.append(i.get_height())\n",
        "\n",
        "    total = sum(totals)\n",
        "\n",
        "    for i in ax.patches:\n",
        "        ax.text(i.get_x() - .01, i.get_height() + .5,\n",
        "                str(round((i.get_height() / total) * 100, 2)) + '%', fontsize=12)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5QsWUEsIRm08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Categorical Variables:**\n",
        "\n",
        " * The variables listed in categorical_variables are discrete in nature and represent different categories or factors that could potentially influence sales in Rossmann stores. These variables include factors like the day of the\n",
        " week, whether the store was open, whether a promotion was running, etc.\n",
        "\n",
        "**2. Comparison Across Categories:**\n",
        "\n",
        "* Bar plots are excellent for comparing the sales across different categories within each variable. Each bar represents a category, and the height of the bar corresponds to the average sales for that category. This allows for easy visual comparison of sales performance across different factors.\n",
        "\n",
        "**3. Palette Choice:**\n",
        "\n",
        "* A specific palette ('hsv') was chosen to provide multiple distinct colors for the bars. This enhances the visual appeal of the plots and helps differentiate between the bars corresponding to different categories within each variable.\n",
        "\n",
        "**4. Percentage Labels:**\n",
        "\n",
        "The code also includes percentage labels on each bar to show the contribution of each category to the total sales. This additional information aids in understanding the relative importance of each category in driving sales.\n",
        "\n",
        "**5. Iterative Visualization:**\n",
        "\n",
        "* By iterating over each categorical variable and plotting them individually, the code provides a comprehensive view of the relationship between each variable and sales. This approach allows for focused analysis of each factor's impact on sales.\n",
        "\n",
        "**Overall, the choice of a bar plot in this context is well-suited for visually exploring how various categorical factors influence sales in Rossmann stores. It allows for clear comparisons and insights into the sales performance across different categories.**"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  There were more sales on Monday, probably because shops generally remain closed on Sundays.\n",
        "* It could be seen that the Promo leads to more sales.\n",
        "* Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None. Lowest of Sales were seen on state holidays especially on Christmas.\n",
        "* More stores were open on School Holidays than on State Holidays and hence had more sales than State Holidays.\n",
        "* On an average Store type B had the highest sales.\n",
        "* Highest average sales were seen with Assortment levels-b which is 'extra'.\n",
        "* With Promo2, slightly more sales were seen without it which indicates there are many stores not participating in promo."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained from analyzing the relationship between categorical variables and sales in Rossmann stores can indeed help create a positive business impact. Here's how:\n",
        "\n",
        "**Optimized Promotional Strategies:**\n",
        "\n",
        "* Understanding that promotions lead to higher sales can help Rossmann optimize its promotional strategies. By allocating resources effectively to promote products during periods when sales are typically higher, Rossmann can increase revenue and profitability.\n",
        "\n",
        "**Store Operation Management:**\n",
        "\n",
        "* Insights about the impact of state holidays on sales can guide store operation management. Rossmann can adjust staffing levels and store opening hours accordingly to optimize resource utilization and minimize costs during low-sales periods.\n",
        "\n",
        "**Product Assortment Planning:**\n",
        "\n",
        "* Knowledge about the association between sales and store types, as well as assortment levels, can inform product assortment planning. Rossmann can prioritize stocking products that are popular among customers and align with the preferences of each store's target demographic, leading to increased customer satisfaction and sales.\n",
        "\n",
        "**Strategic Decision Making:**\n",
        "\n",
        "* Insights into factors such as the effectiveness of Promo2 and sales patterns on different days of the week can inform strategic decision-making. Rossmann can allocate resources strategically, focusing on initiatives that have the highest potential for driving sales and improving overall business performance.\n",
        "\n",
        "**Competitive Advantage:**\n",
        "\n",
        "* By leveraging these insights to optimize operations, promotions, and product offerings, Rossmann can gain a competitive advantage in the retail market. A data-driven approach to decision-making can help Rossmann stay ahead of competitors by continuously adapting to changing market dynamics and customer preferences.\n",
        "\n",
        "**Overall, the insights gained from analyzing sales data can enable Rossmann to make informed decisions that enhance operational efficiency, increase sales, and ultimately drive positive business outcomes.**"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open and storetype relationship\n",
        "#this indicates - Open suggests that whether the store was open or closed for refurbishment and weekends or holidays\n",
        "sns.barplot(x=df[\"Open\"],y=df['Sales'],hue=df[\"DayOfWeek\"])"
      ],
      "metadata": {
        "id": "6AWatyl1APJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets see open, how many shops are open on which days\n",
        "#and this gives a counts of stores closed for refurbishment and suggests that most stores are closed on sunday\n",
        "sns.countplot(x=df[\"Open\"], hue=df[\"DayOfWeek\"])"
      ],
      "metadata": {
        "id": "GMVPlT_AAO6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "This is a count plot of open shops according to the day of the week. It's clear that the number of shops open on Sundays were very less and hence low sales. Some shops were closed on weekdays as well accounting to the stores closed due to refurbishment or holidays."
      ],
      "metadata": {
        "id": "zMMe537fAdxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the relationship between store type, assortment levels and sales\n",
        "sns.barplot(x=df[\"StoreType\"],y=df['Sales'],hue=df[\"Assortment\"])"
      ],
      "metadata": {
        "id": "_GNDFuXqAOlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "The above bar plot shows that the store types a, c and d have only assortment level a and c. On the other hand the store type b has all the three kinds of assortment strategies, a reason why average sales were high for store type b stores."
      ],
      "metadata": {
        "id": "vb_K7oy8AlDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Store Type and Sales Exploration\n",
        "store_type = df.groupby(\"StoreType\")[\"Sales\",\"Customers\"].sum().reset_index()\n",
        "store_type.sort_values([\"Sales\",\"Customers\"], ascending= False, inplace = True) # sorting into descending order to get higher values\n",
        "store_type"
      ],
      "metadata": {
        "id": "6NIas1GUAysr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's explore store type a bit and it's influence on sales\n",
        "df.groupby(\"StoreType\")[\"Sales\"].sum().plot.pie(title='Store Type and Sales', legend=True, autopct='%1.1f%%', shadow=True)\n",
        "plt.show()\n",
        "#customers and store type\n",
        "df.groupby(\"StoreType\")[\"Customers\"].sum().plot.pie(title='Customer Share', legend=True, autopct='%1.1f%%', shadow=True)\n",
        "plt.show()\n",
        "#store types in all of the dataset\n",
        "df[\"StoreType\"].value_counts().plot.pie(title='Share of Store Types', legend=True, autopct='%1.1f%%', shadow=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ibpyhnbcA53_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "* A bar plot represents an estimate of central tendency for a numeric variable with the height of each rectangle. Earlier it was seen that the store type b had the highest sales on an average because the default estimation function to the barplot is mean.\n",
        "* But upon further exploration it can be clearly observed that the highest sales belonged to the store type a due to the high number of type a stores in our dataset. Store type a and c had a similar kind of sales and customer share.\n",
        "* Interesting insight to note is that store type b with highest average sales and per store revenue generation looks healthy and a reason for that would be all three kinds of assortment strategies involved which was seen earlier.\n",
        "\n"
      ],
      "metadata": {
        "id": "1MsIDFsaBEgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Continuous Features:"
      ],
      "metadata": {
        "id": "CbL84p7vBNaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sales with Time"
      ],
      "metadata": {
        "id": "O9JZoI-aBT1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "years"
      ],
      "metadata": {
        "id": "FpjEPr4RCgKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sales over the years\n",
        "sales_df_2013 = df[df['Year']== 2013]\n",
        "sales_df_2014 = df[df['Year']==2014]\n",
        "sales_df_2015 = df[df['Year']== 2015]"
      ],
      "metadata": {
        "id": "xIarcxqNBMes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#monthly sales\n",
        "sales_2013 = sales_df_2013.groupby('Month')['Sales'].sum().reset_index()\n",
        "sales_2014 = sales_df_2014.groupby('Month')['Sales'].sum().reset_index()\n",
        "sales_2015 = sales_df_2015.groupby('Month')['Sales'].sum().reset_index()"
      ],
      "metadata": {
        "id": "lhgDXO4TCtdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting\n",
        "plt.plot(sales_2013.loc[:,'Sales'],label='2013',color='orange')\n",
        "plt.plot(sales_2014.loc[:,'Sales'],label='2014',color='blue')\n",
        "plt.plot(sales_2015.loc[:,'Sales'],label='2015',color='green')\n",
        "plt.title('Monthly Sales Over Years')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "v5VMeAyECtIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "Sales rise up by the end of the year before the holidays. Sales for 2014 went down there for a couple months - July to September, indicating stores closed due to refurbishment."
      ],
      "metadata": {
        "id": "xn0FqnlnC1u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scatterplot of Customers and Sales\n",
        "sns.scatterplot(x=df['Customers'], y=df['Sales'])"
      ],
      "metadata": {
        "id": "NerepEtzC6UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "Sales and Customer scatter plot shows a direct positive relation between them with a few outliers."
      ],
      "metadata": {
        "id": "dKhf588oC-SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scatterplot of Competition Distance and Sales\n",
        "sns.scatterplot(x=df['CompetitionDistance'], y=df['Sales'])"
      ],
      "metadata": {
        "id": "Fhha--6xDBOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "From the above scatter plot it can be observed that mostly the competitor stores weren't that far from each other and the stores densely located near each other saw more sales."
      ],
      "metadata": {
        "id": "9v4EewZZDFdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution plot of Sales, as expected positively skewed\n",
        "sns.distplot(x=df['Sales'], hist_kws={'color': 'blue'}, kde_kws={'color': 'red'})"
      ],
      "metadata": {
        "id": "GXXyzz5WDKGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "The drop in sales indicates the 0 sales accounting to the stores temporarily closed due to refurbishment. This drop was also seen in the Sales over the years plot earlier."
      ],
      "metadata": {
        "id": "Pd7wbsdREPSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Correlation Matrix\n",
        "Correlation is a statistical term used to measure the degree in which two variables move in relation to each other. A perfect positive correlation means that the correlation coefficient is exactly 1. This implies that as one variable moves, either up or down, the other moves in the same direction. A perfect negative correlation means that two variables move in opposite directions, while a zero correlation implies no linear relationship at all.\n",
        "\n",
        "By checking the correlation the factors affecting sales can be figured out."
      ],
      "metadata": {
        "id": "TL4mI19TEW4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we need only meaningful numeric columns here, let's drop the unnecessary to get a clear picture\n",
        "columns_to_drop = ['Store', 'Year', 'WeekOfYear', 'DayOfYear']\n",
        "corr_df = df.drop(columns = columns_to_drop, axis =1)\n",
        "corr_df['StateHoliday'].replace({'a':1, 'b':1,'c':1}, inplace=True)"
      ],
      "metadata": {
        "id": "qunjNaYQETSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation heatmap\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.heatmap(corr_df.corr(), cmap=\"coolwarm\", annot=True)"
      ],
      "metadata": {
        "id": "1DzBrlr_Ea8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "* Day of the week has a negative correlation indicating low sales as the weekends, and promo, customers and open has positive correlation.\n",
        "* State Holiday has a negative correlation suggesting that stores are mostly closed on state holidays indicating low sales.\n",
        "* CompetitionDistance showing negative correlation suggests that as the distance increases sales reduce, which was also observed through the scatterplot earlier.\n",
        "* There's multicollinearity involved in the dataset as well. The features telling the same story like Promo2, Promo2 since week and year are showing multicollinearity.\n",
        "* The correlation matrix is agreeing with all the observations done earlier while exploring through barplots and scatterplots.\n"
      ],
      "metadata": {
        "id": "x8yR3fQ2Elha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EDA Conclusions and Hypotheses Validation:\n",
        "* It makes sense that there is a positive association between sales and customers.\n",
        "\n",
        "* From this, it can be inferred that Monday's sales were higher than Sunday's, which had the lowest weekly sales. This is likely due to the fact that stores are typically closed on Sundays. This supports the theory on this trait.\n",
        "\n",
        "* It is evident that promotions have a favourable impact on sales and customers.\n",
        "\n",
        "* It is evident that during state and school holidays, the majority of the stores stay closed.\n",
        "It's crucial to remember, though, that more stores were open on school holidays than on state holidays, and as a result, sales were higher on school holidays.\n",
        "\n",
        "* Store types 'b' and 'd' appear to have a lot more opportunities based on the statistics above, as they had higher sales per customer and a higher number of customers per store, respectively. Because store types A and C accounted for the bulk of the stores, they had the highest overall income figures despite having very identical \"per customer and per store\" sales figures. Store Type B, on the other hand, was extremely small in number, but even so, its average sales were higher than those of the others.\n",
        "\n",
        "* It was previously noted that only store type B had all three types of assortment levels, while the other store types only had two. Because each B type store has a substantially higher revenue than the others, it appears that the products in those stores are distinct from those in the others.\n",
        "\n",
        "* A comparison of the three years' sales shows that they rise towards the end of the year, suggesting that more people are shopping in advance of the holidays. There was Christmas seasonality at every store. This supports the earlier theory.\n",
        "The second observation was a brief decline in sales in 2014, which was explained by the closure of some outlets for renovations.\n",
        "\n",
        "* The majority of stores had higher sales than stores farther away and had competition distances between 0 and 10 km.\n"
      ],
      "metadata": {
        "id": "EzL82zKPE4rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Null Hypothesis (H0):** There is no significant difference in sales between stores open on Sundays and stores closed on Sundays.\n",
        "* **Alternate Hypothesis (H1):** Stores open on Sundays have significantly higher sales compared to stores closed on Sundays."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform an independent samples t-test to compare the mean sales of stores open on Sundays versus stores closed on Sundays.\n",
        "\n",
        "To perform an independent samples t-test to compare the mean sales of stores open on Sundays versus stores closed on Sundays, we need to follow these steps:\n",
        "\n",
        "**Formulate Hypotheses:**\n",
        "\n",
        "* **Null Hypothesis (H0):** There is no significant difference in sales between stores open on Sundays and stores closed on Sundays.\n",
        "\n",
        "* Alternate Hypothesis (H1):** Stores open on Sundays have significantly higher sales compared to stores closed on Sundays.\n",
        "\n",
        "**Select Significance Level:**\n",
        "\n",
        "* We'll choose a significance level (alpha) of 0.05.\n",
        "\n",
        "**Perform the T-test:**\n",
        "\n",
        "* We'll use the scipy.stats.ttest_ind() function to perform the independent samples t-test.\n",
        "\n",
        "**Interpret the Results:**\n",
        "\n",
        "* If the p-value is less than alpha, we reject the null hypothesis, indicating a significant difference in sales. Otherwise, we fail to reject the null hypothesis.\n"
      ],
      "metadata": {
        "id": "-HA-UbsPImaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Separate data for stores open and closed on Sundays\n",
        "open_sundays_sales = df[df['DayOfWeek'] == 7]['Sales']\n",
        "closed_sundays_sales = df[df['DayOfWeek'] != 7]['Sales']\n",
        "\n",
        "# Perform independent samples t-test\n",
        "t_statistic, p_value = ttest_ind(open_sundays_sales, closed_sundays_sales)\n",
        "\n",
        "# Print results\n",
        "print(\"Independent Samples T-Test Results:\")\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in sales between stores open and closed on Sundays.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in sales between stores open and closed on Sundays.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform an independent samples t-test to compare the mean sales of stores open on Sundays versus stores closed on Sundays.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical test chosen for comparing the mean sales of stores open on Sundays versus stores closed on Sundays is the independent samples t-test. Here's why this test was chosen:\n",
        "\n",
        "1. **Independent Samples:** The samples (sales data) for stores open on Sundays and stores closed on Sundays are independent of each other. The sales of one store being open on Sunday does not affect the sales of another store being closed on Sunday.\n",
        "\n",
        "2. **Two Groups:** There are two distinct groups being compared: stores open on Sundays and stores closed on Sundays.\n",
        "\n",
        "3. **Continuous Outcome:** Sales data is continuous and numerical, making it suitable for the t-test.\n",
        "\n",
        "4. **Normal Distribution Assumption:** The t-test assumes that the distributions of the sales data in the two groups are approximately normal. While this assumption may not be perfectly met, the t-test is robust to moderate violations of normality, especially with large sample sizes.\n",
        "\n",
        "5. **Equal Variances Assumption:** The t-test assumes equal variances between the two groups. We can verify this assumption using Levene's test or by visually inspecting the data. If the assumption is violated, we can use Welch's t-test, which is a modification of the independent samples t-test that does not assume equal variances.\n",
        "\n",
        "Given these considerations and assumptions, the independent samples t-test is an appropriate choice for comparing the mean sales of stores open on Sundays versus stores closed on Sundays in this scenario."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):** There is no significant difference in sales between stores with and without promotions.\n",
        "\n",
        "**Alternate Hypothesis (H1):** Stores with promotions have significantly higher sales compared to stores without promotions."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform an independent samples t-test to compare the mean sales of stores with promotions versus stores without promotions.\n",
        "\n",
        "To perform an independent samples t-test to compare the mean sales of stores with promotions versus stores without promotions, we follow these steps:\n",
        "\n",
        "1. **Formulate Hypotheses:**\n",
        "\n",
        "* **Null Hypothesis (H0):** There is no significant difference in sales between stores with promotions and stores without promotions.\n",
        "\n",
        "* **Alternate Hypothesis (H1):** Stores with promotions have significantly higher sales compared to stores without promotions.\n",
        "\n",
        "2. **Select Significance Level:**\n",
        "\n",
        "* We'll choose a significance level (alpha) of 0.05.\n",
        "\n",
        "3. **Perform the T-test:**\n",
        "\n",
        "* We'll use the scipy.stats.ttest_ind() function to perform the independent samples t-test.\n",
        "\n",
        "4. **Interpret the Results:**\n",
        "\n",
        "If the p-value is less than alpha, we reject the null hypothesis, indicating a significant difference in sales. Otherwise, we fail to reject the null hypothesis."
      ],
      "metadata": {
        "id": "Rm55JjXjKv34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Separate data for stores with and without promotions\n",
        "stores_with_promotions_sales = df[df['Promo'] == 1]['Sales']\n",
        "stores_without_promotions_sales = df[df['Promo'] == 0]['Sales']\n",
        "\n",
        "# Perform independent samples t-test\n",
        "t_statistic, p_value = ttest_ind(stores_with_promotions_sales, stores_without_promotions_sales)\n",
        "\n",
        "# Print results\n",
        "print(\"Independent Samples T-Test Results:\")\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in sales between stores with promotions and stores without promotions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in sales between stores with promotions and stores without promotions.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed an independent samples t-test to compare the mean sales of stores with promotions versus stores without promotions."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical test chosen for comparing the mean sales of stores with promotions versus stores without promotions is the independent samples t-test. Here's why this test was chosen:\n",
        "\n",
        "1. **Independent Samples:** The samples (sales data) for stores with promotions and stores without promotions are independent of each other. The sales of one store with promotions do not affect the sales of another store without promotions.\n",
        "\n",
        "2. **Two Groups:** There are two distinct groups being compared: stores with promotions and stores without promotions.\n",
        "\n",
        "3. **Continuous Outcome:** Sales data is continuous and numerical, making it suitable for the t-test.\n",
        "\n",
        "4. **Normal Distribution Assumption:** The t-test assumes that the distributions of the sales data in the two groups are approximately normal. While this assumption may not be perfectly met, the t-test is robust to moderate violations of normality, especially with large sample sizes.\n",
        "\n",
        "5. **Equal Variances Assumption:** The t-test assumes equal variances between the two groups. We can verify this assumption using Levene's test or by visually inspecting the data. If the assumption is violated, we can use Welch's t-test, which is a modification of the independent samples t-test that does not assume equal variances.\n",
        "\n",
        "Given these considerations and assumptions, the independent samples t-test is an appropriate choice for comparing the mean sales of stores with promotions versus stores without promotions in this scenario."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):** There is no significant difference in sales between different store types.\n",
        "\n",
        "**Alternate Hypothesis (H1):** There is a significant difference in sales between different store types."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform a one-way analysis of variance (ANOVA) test to determine if there is a significant difference in mean sales across different store types.\n",
        "\n",
        "\n",
        "To perform a one-way analysis of variance (ANOVA) test to determine if there is a significant difference in mean sales across different store types, we follow these steps:\n",
        "\n",
        "1. **Formulate Hypotheses:**\n",
        "\n",
        "* **Null Hypothesis (H0):** There is no significant difference in mean sales across different store types.\n",
        "\n",
        "* **Alternate Hypothesis (H1):** There is a significant difference in mean sales across different store types.\n",
        "\n",
        "2. **Select Significance Level:**\n",
        "\n",
        "* We'll choose a significance level (alpha) of 0.05.\n",
        "\n",
        "3. **Perform the ANOVA Test:**\n",
        "\n",
        "* We'll use the scipy.stats.f_oneway() function to perform the ANOVA test.\n",
        "\n",
        "4. **Interpret the Results:**\n",
        "\n",
        "* If the p-value is less than alpha, we reject the null hypothesis, indicating a significant difference in mean sales across different store types. Otherwise, we fail to reject the null hypothesis."
      ],
      "metadata": {
        "id": "UVdRxBNVNYVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Separate sales data for different store types\n",
        "store_type_sales = []\n",
        "for store_type in df['StoreType'].unique():\n",
        "    store_type_sales.append(df[df['StoreType'] == store_type]['Sales'])\n",
        "\n",
        "# Perform one-way ANOVA test\n",
        "f_statistic, p_value = f_oneway(*store_type_sales)\n",
        "\n",
        "# Print results\n",
        "print(\"One-Way ANOVA Test Results:\")\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret results\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in mean sales across different store types.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in mean sales across different store types.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed a one-way analysis of variance (ANOVA) test to determine if there is a significant difference in mean sales across different store types."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical test chosen for comparing the mean sales across different store types is the one-way analysis of variance (ANOVA) test. Here's why this test was chosen:\n",
        "\n",
        "1. **Multiple Groups:** We have multiple groups (store types: A, B, C, D) being compared simultaneously.\n",
        "\n",
        "2. **Continuous Outcome:** Sales data is continuous and numerical, making it suitable for ANOVA.\n",
        "\n",
        "3. **Independence:** The sales data for each store type is assumed to be independent of the sales data for other store types.\n",
        "\n",
        "4. **Normal Distribution Assumption:** ANOVA assumes that the residuals (deviations of individual data points from the group means) are normally distributed. While this assumption may not be perfectly met, ANOVA is robust to moderate violations of normality, especially with large sample sizes.\n",
        "\n",
        "5. **Homogeneity of Variances:** ANOVA assumes that the variances of sales across different store types are approximately equal. We can verify this assumption using Levene's test or by visually inspecting the data.\n",
        "\n",
        "Given these considerations and assumptions, the one-way ANOVA test is an appropriate choice for comparing the mean sales across different store types in this scenario. It allows us to determine if there is a significant difference in mean sales across the store types while accounting for the variability within each group and providing insights into which groups, if any, have significantly different means."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#no of observations for closed stores with 0 sales\n",
        "(df[df.Open == 0]).shape"
      ],
      "metadata": {
        "id": "kXBwhlUgPC4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "######It is mentioned in the problem statement that some stores were temporarily closed for refurbishment and hence did not generate any sales. This was also indicated in the barplot of Open vs Sales."
      ],
      "metadata": {
        "id": "ALD6xhjUO-04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#since the stores closed had 0 sale value; removing the irrelevant part\n",
        "df1 = df[df.Open != 0]\n",
        "df1.drop('Open', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "uUiN1i_pPKD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "df1.head(1)"
      ],
      "metadata": {
        "id": "Haoj_ZI3PNDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution plot of Sales, as expected positively skewed\n",
        "import seaborn as sns\n",
        "\n",
        "sns.distplot(df1['Sales'], color='green')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VvRElrh2PQpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## transformation\n",
        "df1['Sales'] = np.log(df1['Sales'])"
      ],
      "metadata": {
        "id": "gvJNqanGPQdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "9lVUI2b_Pjma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(df1[df1['Sales'] == float(\"-inf\")].index,inplace=True)"
      ],
      "metadata": {
        "id": "LdBWZWzmPn7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution plot of Sales\n",
        "sns.distplot(df1['Sales'], color='cyan')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KEhq7L_tPnx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing into boolean\n",
        "df1['StateHoliday'].replace({'a':1, 'b':1,'c':1}, inplace=True)"
      ],
      "metadata": {
        "id": "tkbxalfxPnbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining competition open since month and year into total months\n",
        "df1['CompetitionOpen'] = (df1['Year'] - df1['CompetitionOpenSinceYear'])*12 + (df1['Month'] - df1['CompetitionOpenSinceMonth'])\n",
        "#correcting the neg values\n",
        "df1['CompetitionOpen'] = df1['CompetitionOpen'].apply(lambda x:0 if x < 0 else x)\n",
        "#dropping both the columns\n",
        "df1.drop(['CompetitionOpenSinceMonth','CompetitionOpenSinceYear'], axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "AJlOW4JjQMRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing promo2 features into meaningful inputs\n",
        "#combining promo2 to total months\n",
        "df1['Promo2Open'] = (df1['Year'] - df1['Promo2SinceYear'])*12 + (df1['WeekOfYear'] - df1['Promo2SinceWeek'])*0.230137\n",
        "\n",
        "#correcting the neg values\n",
        "df1['Promo2Open'] = df1['Promo2Open'].apply(lambda x:0 if x < 0 else x)*df1['Promo2']\n",
        "\n",
        "#creating a feature for promo interval and checking if promo2 was running in the sale month\n",
        "def promo2running(df):\n",
        "  month_dict = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
        "  try:\n",
        "    months = df['PromoInterval'].split(',')\n",
        "    if df['Month'] and month_dict[df['Month']] in months:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  except Exception:\n",
        "    return 0\n",
        "\n",
        "#Applying\n",
        "df1['Promo2running'] = df1.apply(promo2running,axis=1)*df1['Promo2']\n",
        "\n",
        "#Dropping unecessary columns\n",
        "df1.drop(['Promo2SinceYear','Promo2SinceWeek','PromoInterval'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "Zfc5cpgfQMMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting date and store as index\n",
        "df1.set_index(['Date','Store'],inplace=True)\n",
        "#sorting index following the time series\n",
        "df1.sort_index(inplace=True)"
      ],
      "metadata": {
        "id": "JBHz0HEUQMIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head(1)"
      ],
      "metadata": {
        "id": "TO7vgw4GQL-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Outlier Detection"
      ],
      "metadata": {
        "id": "tBjCj-02QYea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Outliers and Z score:\n",
        "In statistics, an outlier is a data point that differs significantly from other observations. Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution.\n",
        "\n",
        "Z-score is a statistical measure that tells you how far is a data point from the rest of the dataset. In a more technical term, Z-score tells how many standard deviations away a given observation is from the mean.\n",
        "\n",
        "z = (x-mean)/standard deviation"
      ],
      "metadata": {
        "id": "NMvTph0DQcLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code to seperate outliers\n",
        "mean_sales = np.mean(df1['Sales']) #mean\n",
        "sd_sales = np.std(df1['Sales'])   #standard deviation\n",
        "#More than 3 standard deviation is an outlier\n",
        "threshold = 3\n",
        "#code to identify them\n",
        "outliers = []\n",
        "for value in df1['Sales']:\n",
        "    z_score = (value-mean_sales)/sd_sales\n",
        "    if z_score > threshold:\n",
        "        outliers.append(value)\n",
        "#total no of outliers\n",
        "print(f'Total number of Outliers present in the Sales column are {len(outliers)}.')\n",
        "#plotting the outlier distribution\n",
        "sns.distplot(x=outliers).set(title='Outliers Distribution')\n",
        "\n",
        "sns.distplot(outliers, color='magenta').set(title='Outliers Distribution')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ebqpZEdqQdbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data points with sales value higher than 10.2 are very low and hence they an be considered as outliers. The percentage of outliers in our dataset:"
      ],
      "metadata": {
        "id": "YZqh-apGRg73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#percentage of sales greater than 10.2\n",
        "sales_outliers = df1.loc[df1['Sales']> 10.2]\n",
        "percentage_of_outliers = (len(sales_outliers)/len(df1))*100\n",
        "#print\n",
        "print(f'The percentage of observations of sales greater than 28000 are {percentage_of_outliers}')"
      ],
      "metadata": {
        "id": "GPX7MH6oRhxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#exploring the reasons behind this behaviour\n",
        "sales_outliers"
      ],
      "metadata": {
        "id": "nHxQv5ivRihX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "Some interesting insights can be drawn from these outliers dataframe:\n",
        "* First thing that comes to notice is the DayOfWeek for Store 262. It's sunday and it has high sales and it's of the store type B.\n",
        "* All other data points had promotion going on and they had a high number of Customers as well indicating no absurd behavior.\n",
        "* It can be well established that the outliers are showing this behavior for the stores with promotion = 1 and store type B. It would not be wise to treat them because the reasons behind this behavior seems fair.\n"
      ],
      "metadata": {
        "id": "y4Jcf5MASeJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets see which stores were open on Sunday in the outliers dataframe\n",
        "#store 262\n",
        "sales_outliers.loc[sales_outliers['DayOfWeek']==7]"
      ],
      "metadata": {
        "id": "0QZ-zuo9Ry1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's explore store type and Day Of week\n",
        "sns.barplot(x=df1['DayOfWeek'],y=df1[\"Sales\"],hue=df1['StoreType'])"
      ],
      "metadata": {
        "id": "OMR5BdFFSoaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's verify in the full dataset\n",
        "df1.loc[(df1['DayOfWeek']==7) & (df1['StoreType']=='b')]"
      ],
      "metadata": {
        "id": "l-HIdVD0T-o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "* This suggests that store type b had high sales almost all week. No store of type C was open on Sunday.\n",
        "* Being open 24*7 along with all kinds of assortments available is probably the reason why it had higher average sales than any other store type.\n"
      ],
      "metadata": {
        "id": "UbQCb0SLUFuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "df1.shape"
      ],
      "metadata": {
        "id": "TsBaS1nOUCQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outlier Treatment**\n",
        "\n",
        "- It can be well established that the outliers are showing this behaviour for the stores with promotion = 1 and store type B. It would not be wise to treat them because the reasons behind this behaviour seems fair and important from the business point of view.\n",
        "- The primary reasons for the behaviour are promotion and store type B.\n",
        "- If the outliers are a valid occurrence it would be wise not to treat them by deleting or manipulating them especially when we have established the ups and downs of the target variable in relation to the other features. It is well established that there is seasonality involved and no linear relationship is possible to fit. For these kinds of datasets tree based machine learning algorithms are used which are robust to outlier effect."
      ],
      "metadata": {
        "id": "6yt9qg30UUpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just in case something messes up\n",
        "df2 = df1.copy()"
      ],
      "metadata": {
        "id": "WMrdn4E4UooL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(1)"
      ],
      "metadata": {
        "id": "a2-bjuGuUolV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sales should be the last col\n",
        "columns=list(df2.columns)\n",
        "columns.remove('Sales')\n",
        "columns.append('Sales')\n",
        "df2=df2[columns]"
      ],
      "metadata": {
        "id": "gLU344LwUoco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check\n",
        "df2.head(1)"
      ],
      "metadata": {
        "id": "cydlG9x3UvQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UNAnoMqQUSGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modeling\n",
        "**Factors affecting in choosing the model:**\n",
        "\n",
        "Determining which algorithm to use depends on many factors like the problem statement and the kind of output you want, type and size of the data, the available computational time, number of features, and observations in the data, to name a few.\n",
        "\n",
        "The dataset used in this analysis has:\n",
        "- A multivariate time series relation with sales and hence a linear relationship cannot be assumed in this analysis. This kind of dataset has patterns such as peak days, festive seasons etc which would most likely be considered as outliers in simple linear regression.\n",
        "- Having X columns with 30% continuous and 70% categorical features. Business prefers the model to be interpretable in nature and decision based algorithms work better with categorical data."
      ],
      "metadata": {
        "id": "jVjNuxvuXsak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train-Test Split"
      ],
      "metadata": {
        "id": "bOO6YbHPXyJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start date\n",
        "df2.head(1)"
      ],
      "metadata": {
        "id": "mPwyyWb_XzQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#end date\n",
        "df2.tail(1)"
      ],
      "metadata": {
        "id": "T1A6_268X4J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we won't need customers for sales forecasting\n",
        "df2.drop('Customers',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "_Oa0YJsRX7fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#slicing the most recent six weeks and creating train and test set\n",
        "#train\n",
        "start_train = pd.to_datetime(\"2013-01-01\")\n",
        "end_train = pd.to_datetime(\"2015-06-14\")\n",
        "df_train = df2.loc[start_train:end_train]\n",
        "#test\n",
        "start_test = pd.to_datetime(\"2015-06-15\")\n",
        "end_test = pd.to_datetime(\"2015-07-31\")\n",
        "df_test = df2.loc[start_test:end_test]"
      ],
      "metadata": {
        "id": "QhvOI_m3X-3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#csv\n",
        "df_train1 = df_train.to_csv(\"/content/drive/MyDrive/Rossman ML Training, Test dataset/df_train.csv\")\n",
        "df_test2 = df_test.to_csv(\"/content/drive/MyDrive/Rossman ML Training, Test dataset/df_test.csv\")"
      ],
      "metadata": {
        "id": "CY-a-27yYNjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X and y split for train and test\n",
        "X_train = df_train.drop('Sales',axis=1)\n",
        "y_train = df_train[['Sales']]\n",
        "X_test = df_test.drop('Sales',axis=1)\n",
        "y_test = df_test[['Sales']]\n",
        "print(f'The shape of X_train is: {X_train.shape}')\n",
        "print(f'The shape of y_train is: {y_train.shape}')\n",
        "print(f'The shape of X_test is: {X_test.shape}')\n",
        "print(f'The shape of y_test is: {y_test.shape}')"
      ],
      "metadata": {
        "id": "nRqdGr_LYuNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#categorical features\n",
        "categorical_cols = ['DayOfWeek', 'StoreType', 'Assortment']\n",
        "\n",
        "#fit encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# train\n",
        "X_train_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
        "encoded_features = [col + '_' + str(category) for col, categories in zip(categorical_cols, encoder.categories_) for category in categories]\n",
        "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_features, index=X_train.index)\n",
        "X_train = pd.concat([X_train, X_train_encoded_df], axis=1)\n",
        "X_train.drop(categorical_cols, axis=1, inplace=True)\n",
        "\n",
        "# test\n",
        "X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
        "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_features, index=X_test.index)\n",
        "X_test = pd.concat([X_test, X_test_encoded_df], axis=1)\n",
        "X_test.drop(categorical_cols, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "JETuBeySZl5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "stdsc = StandardScaler()\n",
        "X_train[list(X_train.columns)] = stdsc.fit_transform(X_train[list(X_train.columns)])\n",
        "X_test[list(X_test.columns)] = stdsc.transform(X_test[list(X_test.columns)])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "y_train[list(y_train.columns)] = scaler.fit_transform(y_train[list(y_train.columns)])\n",
        "y_test[list(y_test.columns)] = scaler.transform(y_test[list(y_train.columns)])"
      ],
      "metadata": {
        "id": "8Ne_qeEXYt__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Baseline Model: Decision Tree\n",
        "A baseline is a simple model that provides reasonable results on a task and does not require much expertise and time to build. Decision trees are used for handling non-linear data sets effectively.\n"
      ],
      "metadata": {
        "id": "0lCsfvvDaA1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "#metrics import\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "AMZCFks-Ytzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting decision tree\n",
        "dt_basic = DecisionTreeRegressor(random_state=42)\n",
        "dt_basic.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "55x3Qv1uaH_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Columns needed to compare metrics\n",
        "comparison_columns = ['Model_Name', 'Train_MAE', 'Train_MSE', 'Train_RMSE', 'Train_R2', 'Train_Adj_R2' ,'Test_MAE', 'Test_MSE', 'Test_RMSE', 'Test_R2', 'Test_Adj_R2']"
      ],
      "metadata": {
        "id": "WO-V1iRyaH9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to evaluate the model\n",
        "def model_evaluation(model_name,model_variable,X_train,y_train,X_test,y_test):\n",
        "  ''' This function predicts and evaluates various models for regression algorithms, visualizes results\n",
        "      and creates a dataframe that compares the various models.'''\n",
        "\n",
        "  #making predictions\n",
        "  y_pred_train = model_variable.predict(X_train)\n",
        "  y_pred_test = model_variable.predict(X_test)\n",
        "\n",
        "  # Plot the test results\n",
        "  a = y_test.copy()\n",
        "  a['Pred Sales'] = y_pred_test.tolist()\n",
        "  df_plot = a.reset_index(level=['Date'])\n",
        "  plot = df_plot.groupby('Date')['Sales','Pred Sales'].sum()\n",
        "  sns.lineplot(data = plot)\n",
        "  plt.ylabel(\"Total Sales and Predicted Sales\")\n",
        "  plt.xticks(rotation = 25)\n",
        "\n",
        "  #calculate metrics and print the results for test set\n",
        "  #Mean Absolute Error or MAE\n",
        "  MAE_train = round(mean_absolute_error(y_train,y_pred_train),6)\n",
        "  MAE_test = round(mean_absolute_error(y_test,y_pred_test),6)\n",
        "  #Mean Squared Error or MSE\n",
        "  MSE_train = round(mean_squared_error(y_train,y_pred_train),6)\n",
        "  MSE_test = round(mean_squared_error(y_test,y_pred_test),6)\n",
        "  #Root Mean Squared Error or RMSE\n",
        "  RMSE_train = round(mean_squared_error(y_train,y_pred_train,squared=False),6)\n",
        "  RMSE_test = round(mean_squared_error(y_test,y_pred_test,squared=False),6)\n",
        "  #R2\n",
        "  R2_train = round(r2_score(y_train, y_pred_train),6)\n",
        "  R2_test = round(r2_score(y_test, y_pred_test),6)\n",
        "  #Adjusted R2\n",
        "  Adj_r2_train = round(1 - (1-r2_score(y_train, y_pred_train)) * (len(y_train)-1)/(len(y_train)-X_train.shape[1]-1),6)\n",
        "  Adj_r2_test = round(1 - (1-r2_score(y_test, y_pred_test)) * (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1),6)\n",
        "  #printing test results\n",
        "  print(f'The Mean Absolute Error for the validation set is {MAE_test}')\n",
        "  print(f'The Mean Squared Error for the validation set is {MSE_test}')\n",
        "  print(f'The Root Mean Squared Error for the validation set is {RMSE_test}')\n",
        "  print(f'The R^2 for the validation set is {R2_test}')\n",
        "  print(f'The Adjusted R^2 for the validation set is {Adj_r2_test}')\n",
        "\n",
        "  #Saving our results\n",
        "  global comparison_columns\n",
        "  metric_scores = [model_name,MAE_train,MSE_train,RMSE_train,R2_train,Adj_r2_train,MAE_test,MSE_test,RMSE_test,R2_test,Adj_r2_test]\n",
        "  final_dict = dict(zip(comparison_columns,metric_scores))\n",
        "  return [final_dict]"
      ],
      "metadata": {
        "id": "B2UNrYitaH6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to create the comparison table\n",
        "final_list = []\n",
        "def add_list_to_final_df(dict_list):\n",
        "  global final_list\n",
        "  for elem in dict_list:\n",
        "    final_list.append(elem)\n",
        "  global comparison_df\n",
        "  comparison_df = pd.DataFrame(final_list, columns= comparison_columns)"
      ],
      "metadata": {
        "id": "8YIujIqzaH3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decision tree evaluation\n",
        "decision_tree = model_evaluation('Decision Tree Regressor',dt_basic,X_train,y_train,X_test,y_test)"
      ],
      "metadata": {
        "id": "-tmeg-_LaHzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add results to comparison df\n",
        "add_list_to_final_df(decision_tree)"
      ],
      "metadata": {
        "id": "eZRcZjuEaHv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparison df\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "PCZI5XZRamIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "- The decision tree was selected as the baseline model since the majority of our characteristics were categorical, with only a small number having continuous value. The findings above demonstrate that while a basic decision tree performs admirably on the validation set, it has overfitted the train set to an extreme. For upcoming data points, it is preferable to have a much more generalised model.\n",
        "\n",
        "- Unlike scientific facilities where results are more important than interpretability, businesses prefer their models to be interpretable in order to recognise patterns and adjust their strategies.\n",
        "\n",
        "- When the majority of the features are categorical, it is advantageous to continue with tree-based methods if interpretability is a priority. Tweaked hyperparameters can be used to expand the tree sufficiently deep without overfitting."
      ],
      "metadata": {
        "id": "1hdCmyZYawdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest"
      ],
      "metadata": {
        "id": "YBSDngqLbSKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "A08gfV2cbTEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting\n",
        "random_forest = RandomForestRegressor(n_estimators=100,random_state=42)\n",
        "random_forest.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Lrdq26KQbTtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model evaluation\n",
        "random_f = model_evaluation('Random Forest Regressor',random_forest,X_train,y_train,X_test,y_test)"
      ],
      "metadata": {
        "id": "fmGbb5SFbTq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updating comparison df\n",
        "add_list_to_final_df(random_f)"
      ],
      "metadata": {
        "id": "ZmAW5VIxbToV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparison df\n",
        "comparison_df"
      ],
      "metadata": {
        "id": "UJv-2jGubTlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters used in random forest\n",
        "print('Parameters currently in use:')\n",
        "print(random_forest.get_params())"
      ],
      "metadata": {
        "id": "SXqY-pqIbTjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a csv file for the cleaned dataset\n",
        "# Rossmann sales prediction to be continued in the next notebook, due to lack of ram colab is crashing\n",
        "#creating a csv file for the comparison dataframe\n",
        "results = comparison_df.to_csv(\"/content/drive/MyDrive/Rossman ML Training, Test dataset/results.csv\")"
      ],
      "metadata": {
        "id": "zJyH7Tc8bTgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "W7aSHKHowqT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYyo1pqZx_OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Observation:\n",
        "Random Forest Regressor results were much better than our baseline model. Next we'll try to tune the hyperparameters and check the results."
      ],
      "metadata": {
        "id": "pfarEwl5b24Y"
      }
    }
  ]
}